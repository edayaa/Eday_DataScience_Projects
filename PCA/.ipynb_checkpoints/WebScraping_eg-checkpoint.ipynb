{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install googlemaps\n",
    "# !pip install python-google-places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleplaces import * #GooglePlaces, types, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading json and converting into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# import pandas as pd\n",
    "\n",
    "path = r'C:/my_path'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "list = []\n",
    "big_frame = pd.DataFrame()\n",
    "\n",
    "for file_ in all_files:\n",
    "    df = pd.read_csv(file_, index_col = None, header = 0)\n",
    "    list.append(df)\n",
    "    \n",
    "big_frame = pd.concat(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('GooglePlacesAPI_TestResult1.json') as f:\n",
    "    data1 = json.load(f)\n",
    "    \n",
    "json_normalize(data1['results']).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import urllib.request as urllib2\n",
    "except ImportError:\n",
    "    import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib3\n",
    "# import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.planemapper.com/flights?page=0&sort=FlightID&flight=-&from=-&to=-&company=-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.planemapper.com/flights?page=0&sort=FlightID&flight=-&from=-&to=-&company=-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for page_num in range(1150):\n",
    "    url = \"https://www.planemapper.com/flights?page=\"+str(page_num)+\"&sort=FlightID&flight=-&from=-&to=-&company=-\"\n",
    "    page = requests.get(url, verify=False) #To avoid the verifications\n",
    "    html = page.content\n",
    "    df_list = pd.read_html(html)\n",
    "    df_tmp = df_list[-1]\n",
    "    df = df.append(df_tmp, ignore_index=True)\n",
    "    print(\"Page: \"+str(page_num))\n",
    "\n",
    "df.to_excel('Output.xlsx', index = False)\n",
    "\n",
    "# The above can also be done in one line\n",
    "# pd.read_html(requests.get(<url>).content)[-1].to_csv(<csv file>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for page_num in range(1150):\n",
    "    url = \"https://www.planemapper.com/flights?page=\"+str(page_num)+\"&sort=FlightID&flight=-&from=-&to=-&company=-\"\n",
    "    page = requests.get(url, verify=False) #To avoid the verifications\n",
    "    html = page.content\n",
    "    df_list = pd.read_html(html)\n",
    "    df_tmp = df_list[-1]\n",
    "    df = df.append(df_tmp, ignore_index=True)\n",
    "    print(\"Page: \"+str(page_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('World_flights_db1.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://planefinder.net/data/aircrafts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page = requests.get(url, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get all the links\n",
    "# soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_link = soup.find_all(\"a\")\n",
    "# for i in all_link:\n",
    "#     print(i.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tables=soup.find_all('table')\n",
    "# all_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_table=soup.find('table', class_='wikitable sortable plainrowheaders')\n",
    "# select_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B, C, D, E, F, G = [], [], [], [], [], [],[]\n",
    "\n",
    "for row in select_table.findAll(\"tr\"):\n",
    "    cells = row.findAll('td')\n",
    "    states=row.findAll('th') #To store second column data\n",
    "    if len(cells)==6: #Only extract table body not heading\n",
    "        A.append(cells[0].find(text=True))\n",
    "        B.append(states[0].find(text=True))\n",
    "        C.append(cells[1].find(text=True))\n",
    "        D.append(cells[2].find(text=True))\n",
    "        E.append(cells[3].find(text=True))\n",
    "        F.append(cells[4].find(text=True))\n",
    "        G.append(cells[5].find(text=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>State/UT</th>\n",
       "      <th>Admin_Capital</th>\n",
       "      <th>Legislative_Capital</th>\n",
       "      <th>Judiciary_Capital</th>\n",
       "      <th>Year_Capital</th>\n",
       "      <th>Former_Capital</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Number, State/UT, Admin_Capital, Legislative_Capital, Judiciary_Capital, Year_Capital, Former_Capital]\n",
       "Index: []"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas to convert list to data frame\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(A,columns=['Number'])\n",
    "df['State/UT']=B\n",
    "df['Admin_Capital']=C\n",
    "df['Legislative_Capital']=D\n",
    "df['Judiciary_Capital']=E\n",
    "df['Year_Capital']=F\n",
    "df['Former_Capital']=G\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "\n",
    "# user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "\n",
    "# url = \"https://www.zomato.com/chennai/restaurants?page=1\"\n",
    "# headers={'User-Agent':user_agent,} \n",
    "\n",
    "# request=urllib.request.Request(url,None,headers) #The assembled request\n",
    "# response = urllib.request.urlopen(request)\n",
    "# data = response.read() # The data u need\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12/13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleplaces import GooglePlaces, types, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR_API_KEY = 'AIzaSyDeBqv3GoMzWK1r-xtF_Os5UFy1gAs09Ao'\n",
    "google_places = GooglePlaces(YOUR_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google_places.nearby_search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = google_places.nearby_search(\n",
    "        location='Chennai', keyword='Restaurant', lat_lng={'lat':'12.984102', 'lng':'80.222684'}, \n",
    "        radius=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_result.has_attributions:\n",
    "    print (query_result.html_attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Place name=\"Vasco's\", lat=13.0164933, lng=80.20514059999999>,\n",
       " <Place name=\"Bombay Brasserie - Adyar\", lat=13.006781, lng=80.256148>,\n",
       " <Place name=\"The Flying Elephant\", lat=13.0108519, lng=80.22339789999999>,\n",
       " <Place name=\"Tuscana Pizzeria\", lat=12.9986696, lng=80.25940709999999>,\n",
       " <Place name=\"IMPIREO RESTAURANT\", lat=12.9886291, lng=80.19529729999999>,\n",
       " <Place name=\"Arun Home Mess & Caterers\", lat=12.978112, lng=80.2289124>,\n",
       " <Place name=\"Murugan Idli\", lat=12.9825639, lng=80.1882401>,\n",
       " <Place name=\"Sangeetha Veg Restaurant\", lat=12.9441966, lng=80.23753839999999>,\n",
       " <Place name=\"Adyar Treat Restaurant\", lat=13.008486, lng=80.25876699999999>,\n",
       " <Place name=\"Bamboo Shoot Restaurant\", lat=12.9942396, lng=80.21739359999999>,\n",
       " <Place name=\"East Coast at Madras Square\", lat=12.956043, lng=80.258866>,\n",
       " <Place name=\"Biswal And Biswal Restaurant\", lat=12.9721826, lng=80.2453158>,\n",
       " <Place name=\"Coastal Aroma - Best Seafood Restaurant in Chennai\", lat=12.9496163, lng=80.2394073>,\n",
       " <Place name=\"Parfait3 Cakes\", lat=12.9790482, lng=80.2267116>,\n",
       " <Place name=\"Rolls Mania\", lat=12.9946687, lng=80.21720259999999>,\n",
       " <Place name=\"Gopalettans Restaurant And Teashop\", lat=12.9840468, lng=80.23080469999999>,\n",
       " <Place name=\"Willows - The Westin\", lat=12.9896451, lng=80.22079939999999>,\n",
       " <Place name=\"Andhikkadai\", lat=12.9825028, lng=80.22466949999999>,\n",
       " <Place name=\"Dindigul Thalappakatti Restaurant\", lat=12.977411, lng=80.21926409999999>,\n",
       " <Place name=\"Karuthapandi\", lat=12.9731988, lng=80.22042329999999>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result.places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vasco's - 4.4 - Poomagal Nagar, Gandhi Nagar, Chennai\n",
      "Bombay Brasserie - Adyar - 4.1 - Villa 77, 1st Main Rd, Gandhi Nagar, Adyar, Chennai\n",
      "The Flying Elephant - 4.4 - Raj Bhavan, 39, Velachery Main Rd, Little Mount, Guindy, Chennai\n",
      "Tuscana Pizzeria - 4 - Shastri Nagar, 3rd Cross, Adyar, Chennai\n",
      "IMPIREO RESTAURANT - 4.2 - ring road, Velachery, no 1, 24th St, Thillaiganga Nagar, Nangainallur, Chennai\n",
      "Arun Home Mess & Caterers - 4.5 - 2nd Cross St, Baby Nagar, Velachery, Chennai\n",
      "Murugan Idli - 4.1 - 22, 4th Main Road, Nanganallur, Chennai\n",
      "Sangeetha Veg Restaurant - 4.1 - Sri Nilayam, 2/96/b, Old Mahabalipuram Rd, Ellaiamman Nagar, Thoraipakkam, Chennai\n",
      "Adyar Treat Restaurant - 4.1 - Old No. 48-B, New No. 103, 1st Main Road, Gandhi Nagar, Adyar, Chennai\n",
      "Bamboo Shoot Restaurant - 4.6 - plot no -7,2nd floor,Ambedkar Nagar(30 meters from phoenix mall, Velachery Main Rd, Velachery, Chennai\n",
      "East Coast at Madras Square - 4.3 - Madras Square 2/520 Sundeep Road\n",
      "Biswal And Biswal Restaurant - 4 - 2nd St, Sunnambu Kolathur, Thiruvengadam Nagar, Perungudi, Chennai\n",
      "Coastal Aroma - Best Seafood Restaurant in Chennai - 4 - 1/155a, 1st Floor, 200 Feet Radial Rd, Thoraipakkam, Chennai\n",
      "Parfait3 Cakes - 4.2 - No.80B, Tharamani Link Road, Next to Karaikudi Restaurant, Velachery, Chennai\n",
      "Rolls Mania - 4.3 - 7,1st floor,Dr Ambedkar Nagar,opposite to ICICI Bank, ,Velachery Main Road, Chennai\n",
      "Gopalettans Restaurant And Teashop - 4.1 - Ambika St, Ayodhya Colony, Velachery, Chennai\n",
      "Willows - The Westin - 4.3 - 154, Velachery Main Rd, Dhadeswaram Nagar, Velachery, Chennai\n",
      "Andhikkadai - 4.3 - No-20, Dandeeshwaram Main Road, Velachery, Chennai\n",
      "Dindigul Thalappakatti Restaurant - 3.9 - 38/1, 100 Feet Bye Pass Road, Velachery Bypass Rd, Chennai\n",
      "Karuthapandi - 3.9 - Adyar anandha bhavan hotels, 37f, Velachery - Tambaram Main Rd, Velachery, Chennai\n"
     ]
    }
   ],
   "source": [
    "for i in query_result.places:\n",
    "    print(i.name, \"-\", i.rating, \"-\", i.vicinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Paradise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Project Paradise'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Project {name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12/13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# import pandas as pd\n",
    "\n",
    "path = r'C:/my_path'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "list = []\n",
    "big_frame = pd.DataFrame()\n",
    "\n",
    "for file_ in all_files:\n",
    "    df = pd.read_csv(file_, index_col = None, header = 0)\n",
    "    list.append(df)\n",
    "    \n",
    "big_frame = pd.concat(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Unable to locate element: .zs-load-more-count\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\my\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\my\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3 days ago'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#Prerequisite: Download Mozilla for corresponding OS from below link\n",
    "# https://github.com/mozilla/geckodriver/releases\n",
    "\"\"\"\n",
    "Created on Fri Jan 18 13:14:27 2019\n",
    "\n",
    "@author: G1024\n",
    "\"\"\"\n",
    "\n",
    "from selenium import webdriver\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "#import re\n",
    "import pandas as pd\n",
    "#from tabulate import tabulate\n",
    "#import os\n",
    "import time\n",
    "\n",
    "#launch url\n",
    "url = \"https://www.zomato.com/chennai/paradise-thiruvanmiyur/reviews\"\n",
    "\n",
    "# create a new Firefox session\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "driver.get(url)\n",
    "\n",
    "python_button = driver.find_element_by_partial_link_text('All Reviews') \n",
    "time.sleep(2)\n",
    "python_button.click() #click fhsu link\n",
    "\n",
    "\n",
    "#loadMore_python_button = driver.find_element_by_class_name('zs-load-more-count') \n",
    "#loadMore_python_button.click() #click fhsu link\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        loadMoreButton = driver.find_element_by_class_name('zs-load-more-count') \n",
    "        time.sleep(2)\n",
    "        loadMoreButton.click()\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "#Selenium hands the page source to Beautiful Soup\n",
    "soup_level1=BeautifulSoup(driver.page_source, 'html')\n",
    "\n",
    "reviewsList = [] #empty list\n",
    "reviewerLevel =[]\n",
    "reviewRatingList =[]\n",
    "dateTime = []\n",
    "timeAgo =[]\n",
    "\n",
    "#for link in soup_level1.find_all(\"div\", {\"class\": \"rev-text\"}):\n",
    "#    print(link.contents[2])\n",
    "#    reviewsList.append(link.contents[2].encode('ascii', 'ignore').strip())\n",
    "    \n",
    "for link in soup_level1.find_all(\"div\", {\"class\": \"res-review-body\"}):\n",
    "    reviewerLevel.append(str(link.find(\"span\",{\"class\": \"grey-text\"}).contents[0]).replace('\\n', '').strip())\n",
    "    reviewRatingList.append(link.find(\"div\", {\"class\":\"ttupper\"})[\"aria-label\"])\n",
    "    reviewsList.append(link.find(\"div\", {\"class\": \"rev-text\"}).contents[2].encode('ascii', 'ignore').strip())\n",
    "    dateTime.append(link.find(\"time\")[\"datetime\"])\n",
    "    timeAgo.append(link.find(\"time\").contents[0])\n",
    "    #reviewsList.append(link.contents[2].encode('ascii', 'ignore').strip())    \n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "type(reviewsList)\n",
    "df = pd.DataFrame({'Reviews':reviewsList, \"ReviewerInfluenceLevel\": reviewerLevel, \"ReviewRating\":reviewRatingList, \"DateAndTime\":dateTime, \"timeAgo\":timeAgo})\n",
    "#link = soup_level1.find(\"div\", {\"class\": \"rev-text\"})\n",
    "#link =str(link)\n",
    "#link.contents[2]\n",
    "#result = re.search(r'title=\"Poor\">Rated</div>(.*?)<div class=\"clear\"></div>', str(link)).group(1)\n",
    "#print(result)\n",
    "df.to_csv(\"C:\\\\Users\\\\my\\\\csv_files\\\\Review.csv\", sep=',')\n",
    "\n",
    "len(reviewsList)\n",
    "\n",
    "link = soup_level1.find(\"div\", {\"class\": \"res-review-body\"})\n",
    "str(link.find(\"span\",{\"class\": \"grey-text\"}).contents[0]).replace('\\n', '').strip()\n",
    "link.find(\"div\", {\"class\":\"ttupper\"})[\"aria-label\"]\n",
    "link.find(\"div\", {\"class\": \"rev-text\"}).contents[2]\n",
    "link.find(\"time\")[\"datetime\"]\n",
    "link.find(\"time\").contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\my'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\my'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('csv_files1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web Scraping links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Access successful.\n",
      "2\n",
      "\n",
      "Access successful.\n",
      "3\n",
      "\n",
      "Access successful.\n",
      "4\n",
      "\n",
      "Access successful.\n",
      "5\n",
      "\n",
      "Access successful.\n",
      "6\n",
      "\n",
      "Access successful.\n",
      "7\n",
      "\n",
      "Access successful.\n",
      "8\n",
      "\n",
      "Access successful.\n",
      "9\n",
      "\n",
      "Access successful.\n",
      "10\n",
      "\n",
      "Access successful.\n",
      "11\n",
      "\n",
      "Access successful.\n",
      "12\n",
      "\n",
      "Access successful.\n",
      "13\n",
      "\n",
      "Access successful.\n",
      "14\n",
      "\n",
      "Access successful.\n",
      "15\n",
      "\n",
      "Access successful.\n",
      "16\n",
      "\n",
      "Access successful.\n",
      "17\n",
      "\n",
      "Access successful.\n",
      "18\n",
      "\n",
      "Access successful.\n",
      "19\n",
      "\n",
      "Access successful.\n",
      "20\n",
      "\n",
      "Access successful.\n",
      "21\n",
      "\n",
      "Access successful.\n",
      "22\n",
      "\n",
      "Access successful.\n",
      "23\n",
      "\n",
      "Access successful.\n",
      "24\n",
      "\n",
      "Access successful.\n",
      "25\n",
      "\n",
      "Access successful.\n",
      "26\n",
      "\n",
      "Access successful.\n",
      "27\n",
      "\n",
      "Access successful.\n",
      "28\n",
      "\n",
      "Access successful.\n",
      "29\n",
      "\n",
      "Access successful.\n",
      "30\n",
      "\n",
      "Access successful.\n",
      "31\n",
      "\n",
      "Access successful.\n",
      "32\n",
      "\n",
      "Access successful.\n",
      "33\n",
      "\n",
      "Access successful.\n",
      "34\n",
      "\n",
      "Access successful.\n",
      "35\n",
      "\n",
      "Access successful.\n",
      "36\n",
      "\n",
      "Access successful.\n",
      "37\n",
      "\n",
      "Access successful.\n",
      "38\n",
      "\n",
      "Access successful.\n",
      "39\n",
      "\n",
      "Access successful.\n",
      "40\n",
      "\n",
      "Access successful.\n",
      "41\n",
      "\n",
      "Access successful.\n",
      "42\n",
      "\n",
      "Access successful.\n",
      "43\n",
      "\n",
      "Access successful.\n",
      "44\n",
      "\n",
      "Access successful.\n",
      "45\n",
      "\n",
      "Access successful.\n",
      "46\n",
      "\n",
      "Access successful.\n",
      "47\n",
      "\n",
      "Access successful.\n",
      "48\n",
      "\n",
      "Access successful.\n",
      "49\n",
      "\n",
      "Access successful.\n",
      "50\n",
      "\n",
      "Access successful.\n",
      "51\n",
      "\n",
      "Access successful.\n",
      "52\n",
      "\n",
      "Access successful.\n",
      "53\n",
      "\n",
      "Access successful.\n",
      "54\n",
      "\n",
      "Access successful.\n",
      "55\n",
      "\n",
      "Access successful.\n",
      "56\n",
      "\n",
      "Access successful.\n",
      "57\n",
      "\n",
      "Access successful.\n",
      "58\n",
      "\n",
      "Access successful.\n",
      "59\n",
      "\n",
      "Access successful.\n",
      "60\n",
      "\n",
      "Access successful.\n",
      "61\n",
      "\n",
      "Access successful.\n",
      "62\n",
      "\n",
      "Access successful.\n",
      "63\n",
      "\n",
      "Access successful.\n",
      "64\n",
      "\n",
      "Access successful.\n",
      "65\n",
      "\n",
      "Access successful.\n",
      "66\n",
      "\n",
      "Access successful.\n",
      "67\n",
      "\n",
      "Access successful.\n",
      "68\n",
      "\n",
      "Access successful.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Python class to scrap link of every restaurant whose zomato page link is given\n",
    "\"\"\"\n",
    "\n",
    "import urllib\n",
    "from urllib import parse\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "from urllib.parse import urlparse\n",
    "import urllib.request\n",
    "from selenium import webdriver\n",
    "from bs4 import NavigableString\n",
    "import sys\n",
    "\n",
    "browser = None\n",
    "\n",
    "try:\n",
    "    browser = webdriver.Firefox()\n",
    "except Exception as error:\n",
    "    print(error)\n",
    "\n",
    "out_file1 = open(\"Vizag_Restaurant_Links_1.txt\", \"ab\")\n",
    "out_file2 = open(\"Vizag_Restaurant_Links_2.txt\", \"ab\")\n",
    "out_file3 = open(\"Vizag_Restaurant_Links_3.txt\", \"ab\")\n",
    "\n",
    "\n",
    "class ZomatoRestaurantLinkGen:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.html_text = None\n",
    "        try:\n",
    "            browser.get(self.url)\n",
    "            self.html_text = browser.page_source\n",
    "            # self.html_text = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "            # self.html_text = requests.get(url).text\n",
    "        except Exception as err:\n",
    "            print(str(err))\n",
    "            return\n",
    "        else:\n",
    "            print('Access successful.')\n",
    "\n",
    "        self.soup = None\n",
    "        if self.html_text is not None:\n",
    "            self.soup = BeautifulSoup(self.html_text, 'lxml')\n",
    "\n",
    "    def scrap(self):\n",
    "        soup = self.soup\n",
    "        for tag in soup.find_all(\"a\", attrs={'data-result-type': 'ResCard_Name'}):\n",
    "            out_file1.write(tag['href'].encode('utf-8').strip() + b'\\n')\n",
    "        for tag in soup.find_all(\"a\", {\"class\": \"search_chain_bottom_snippet\"}):\n",
    "            out_file2.write(tag['href'].encode('utf-8').strip() + b'\\n')\n",
    "        for tag1 in soup.find_all(\"a\", {\"class\":\"ta-right\"}):   \n",
    "            out_file3.write(tag1['href'].encode('utf-8').strip() + b'\\n')\n",
    "         \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if browser is None:\n",
    "        print(\"Selenium not opened\")\n",
    "        sys.exit()\n",
    "\n",
    "    for x in range(1, 69):\n",
    "        print(str(x) + '\\n')\n",
    "        zr = ZomatoRestaurantLinkGen('https://www.zomato.com/visakhapatnam/restaurants?page={}'.format(x))\n",
    "        zr.scrap()\n",
    "    browser.close()\n",
    "    out_file1.close()\n",
    "    out_file2.close()\n",
    "    out_file3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Python class to scrap link of every restaurant whose zomato page link is given\n",
    "\"\"\"\n",
    "\n",
    "import urllib\n",
    "from urllib import parse\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "from urllib.parse import urlparse\n",
    "import urllib.request\n",
    "from selenium import webdriver\n",
    "from bs4 import NavigableString\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "browser = None\n",
    "\n",
    "try:\n",
    "    browser = webdriver.Firefox()\n",
    "except Exception as error:\n",
    "    print(error)\n",
    "\n",
    "#out_file = open(\"chennai_restaurant_details.txt\", \"ab\")\n",
    "\n",
    "out_file4 = open(\"Vizag_Restaurant_Links.txt\", \"ab\")\n",
    "\n",
    "class ZomatoRestaurant:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.html_text = None\n",
    "        try:\n",
    "            browser.get(self.url)\n",
    "            self.html_text = browser.page_source\n",
    "            # self.html_text = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "            # self.html_text = requests.get(url).text\n",
    "        except Exception as err:\n",
    "            print(str(err))\n",
    "            return\n",
    "        else:\n",
    "            print('Access successful.')\n",
    "\n",
    "        self.soup = None\n",
    "        if self.html_text is not None:\n",
    "            self.soup = BeautifulSoup(self.html_text, 'lxml')\n",
    "\n",
    " \n",
    "\n",
    "    def scrap(self):\n",
    "\n",
    "        soup = self.soup\n",
    "\n",
    " \n",
    "        \n",
    "        for tag in soup.find_all(\"a\", attrs={'data-result-type': 'ResCard_Name'}):\n",
    "            out_file4.write(tag['href'].encode('utf-8').strip() + b'\\n')    \n",
    "               \n",
    "          \n",
    "            while True:\n",
    "            \n",
    "                try:    \n",
    "                    nextPageButton =  browser.find_element_by_class_name('next')    \n",
    "                    if(nextPageButton.get_attribute(\"class\") == \"disabled   item next\"):\n",
    "                \n",
    "                        break   \n",
    "                    else:\n",
    "                        nextPageButton.click()\n",
    "                        \n",
    "                        for tag in soup.find_all(\"a\", attrs={'data-result-type': 'ResCard_Name'}):\n",
    "                            out_file4.write(tag['href'].encode('utf-8').strip() + b'\\n')\n",
    "                      \n",
    "                \n",
    "                            #print(link.find(\"a\",{\"class\": \"result-title\"}).contents[0].replace('\\n', '').strip())\n",
    "                except:\n",
    "                    print(\"Error\")\n",
    "                    break\n",
    "\n",
    "\n",
    " \n",
    "            #print(str(tag['href'].encode('utf-8').strip() + b'\\n'))\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    if browser is None:\n",
    "        print(\"Selenium not opened\")\n",
    "        sys.exit()\n",
    "    with open('Vizag_Restaurant_Links_3.txt', 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            zr = ZomatoRestaurant(line)\n",
    "            zr.scrap()\n",
    "    browser.close()\n",
    "    out_file4.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n",
      "Access successful.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Python class to scrap data for a particular restaurant whose zomato link is given\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import urllib\n",
    "from urllib import parse\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import urllib.request\n",
    "from selenium import webdriver\n",
    "from bs4 import NavigableString\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "browser = None\n",
    "try:\n",
    "    browser = webdriver.Firefox()\n",
    "except Exception as error:\n",
    "    print(error)\n",
    "\n",
    "resName = [] \n",
    "resAddress =[]\n",
    "resAllReviews =[]\n",
    "resDeliveryReviews=[]\n",
    "class ZomatoRestaurant:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        # print(\"opening\")\n",
    "        self.html_text = None\n",
    "        try:\n",
    "            browser.get(self.url)\n",
    "            self.html_text = browser.page_source\n",
    "            # self.html_text = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "            # self.html_text = requests.get(url).text\n",
    "        except Exception as err:\n",
    "            print(str(err))\n",
    "            return\n",
    "        else:\n",
    "            print('Access successful.')\n",
    "\n",
    "        self.soup = None\n",
    "        if self.html_text is not None:\n",
    "            self.soup = BeautifulSoup(self.html_text, 'lxml')\n",
    "\n",
    "    def scrap(self):\n",
    "        if self.soup is None:\n",
    "            return {}\n",
    "        soup = self.soup\n",
    "       \n",
    "\n",
    "        try:\n",
    "            resName.append(soup.find(\"a\",{\"class\": [\"large\", \"header\"]}).contents[0].replace('\\n', '').strip())    \n",
    "        except:    \n",
    "            resName.append(\"0\")    \n",
    "        try:\n",
    "            resAddress.append(soup.find(\"div\",{\"class\": \"resinfo-icon\"}))\n",
    "        except:    \n",
    "            resAddress.append(\"0\")    \n",
    "        try:\n",
    "            resAllReviews.append((soup.find(\"a\",attrs={\"data-sort\" : \"reviews-dd\"}).find(\"span\",{\"class\": \"grey-text\"}).contents[0]).replace('\\n', '').strip())    \n",
    "        except:    \n",
    "            resAllReviews.append(\"0\")    \n",
    "        try:\n",
    "            resDeliveryReviews.append((soup.find(\"a\",attrs={\"data-sort\": \"reviews-delivery\"}).find(\"span\",{\"class\": \"grey-text\"}).contents[0]).replace('\\n', '').strip())\n",
    "        except:    \n",
    "            resDeliveryReviews.append(\"0\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if browser is None:\n",
    "        sys.exit()\n",
    "    #out_file = open(\"zomato_bangalore.json\", \"a\")\n",
    "    with open('Vizag_Restaurant_Links.txt', 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            zr = ZomatoRestaurant(line)\n",
    "            zr.scrap()\n",
    "    browser.close()\n",
    "    #df=pd.DataFrame({'ResName':resName})\n",
    "    df = pd.DataFrame({'ResName':resName, \"ResAddress\": resAddress, \"ResAllReviews\":resAllReviews,\"ResDeliveryReviews\":resDeliveryReviews})\n",
    "    \n",
    "    df.to_csv(\"Vizag_Restaurant_ReviewCount3.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
